# Алгоритмическая сложность и анализ производительности

## Содержание

1. [Основы анализа сложности](#основы-анализа-сложности)
   - [О-нотация и её виды](#о-нотация-и-её-виды)
   - [Правила вычисления сложности](#правила-вычисления-сложности)
2. [Временная сложность: основные классы](#временная-сложность-основные-классы)
   - [O(1) — константная сложность](#o1--константная-сложность)
   - [O(log n) — логарифмическая сложность](#olog-n--логарифмическая-сложность)
   - [O(n) — линейная сложность](#on--линейная-сложность)
   - [O(n log n) — линеаризованная логарифмическая сложность](#on-log-n--линеаризованная-логарифмическая-сложность)
   - [O(n²) — квадратичная сложность](#on--квадратичная-сложность)
   - [O(2ⁿ) — экспоненциальная сложность](#o2--экспоненциальная-сложность)
3. [Пространственная сложность](#пространственная-сложность)
4. [Амортизированная сложность](#амортизированная-сложность)
5. [Практические примеры анализа](#практические-примеры-анализа)
6. [Best practices](#best-practices)
7. [Вопросы на собеседовании](#вопросы-на-собеседовании)

## Основы анализа сложности

**Алгоритмическая сложность** — это характеристика эффективности алгоритма, описывающая зависимость между размером входных данных и затрачиваемыми ресурсами (временем выполнения или памятью). Анализ сложности позволяет предсказывать поведение программы при масштабировании и сравнивать различные подходы к решению задачи.

> **Историческая справка**: Концепция асимптотической сложности была формализована в работах Пола Бахмана (1894) и Эдмунда Ландау (начало XX века). Современная O-нотация получила широкое распространение благодаря работе Дональда Кнута в 1970-х годах.

### О-нотация и её виды

**Big O (O-нотация)** описывает верхнюю границу роста функции — наихудший случай выполнения алгоритма. Это наиболее распространённая нотация в индустрии.

**Omega (Ω-нотация)** описывает нижнюю границу — наилучший случай выполнения.

**Theta (Θ-нотация)** описывает точную асимптотическую границу — когда верхняя и нижняя границы совпадают.

В практической разработке обычно используют **Big O**, так как важно понимать наихудший сценарий для гарантий производительности.

### Правила вычисления сложности

1. **Отбрасывание констант**: O(2n) → O(n), O(n/2) → O(n)
2. **Доминирование термов**: O(n² + n) → O(n²), O(n + log n) → O(n)
3. **Умножение вложенных циклов**: два вложенных цикла по n элементов → O(n²)
4. **Сложение последовательных операций**: O(n) + O(m) → O(n + m)
5. **Умножение зависимых операций**: O(n) × O(log n) → O(n log n)

## Временная сложность: основные классы

### O(1) — константная сложность

Операция выполняется за постоянное время независимо от размера входных данных.

**Примеры**:
```java
// Доступ к элементу массива по индексу
int value = array[index]; // O(1)

// Получение значения из HashMap
String value = map.get(key); // O(1) в среднем

// Добавление в конец ArrayList (при наличии места)
list.add(element); // O(1) амортизированная

// Операции со стеком
stack.push(element); // O(1)
stack.pop(); // O(1)
```

**Когда использовать**: Когда требуется гарантированная быстрая операция независимо от размера данных.

### O(log n) — логарифмическая сложность

**Логарифмическая сложность** означает, что при каждом шаге алгоритма размер обрабатываемых данных уменьшается в определённое количество раз (обычно в 2 раза для двоичного логарифма).

> **Важно**: log n растёт очень медленно. Даже для миллиарда элементов log₂(1,000,000,000) ≈ 30 операций.

#### Характеристики log(n) сложности

**Основные свойства**:
- Рост замедляется с увеличением n
- Удвоение размера данных добавляет всего одну дополнительную операцию
- Практически не отличается от O(1) для небольших и средних объёмов данных
- Является признаком эффективных алгоритмов "разделяй и властвуй"

**Таблица роста**:
```
n          | log₂(n) | Прирост
-----------|---------|--------
10         | ~3.3    | -
100        | ~6.6    | +3.3
1,000      | ~10     | +3.4
10,000     | ~13.3   | +3.3
100,000    | ~16.6   | +3.3
1,000,000  | ~20     | +3.4
```

#### Примеры алгоритмов с O(log n)

**1. Бинарный поиск**

Классический пример логарифмической сложности. На каждом шаге отбрасывается половина элементов:

```java
public static int binarySearch(int[] array, int target) {
    int left = 0;
    int right = array.length - 1;
    
    while (left <= right) {
        int mid = left + (right - left) / 2; // избегаем переполнения
        
        if (array[mid] == target) {
            return mid; // найден
        } else if (array[mid] < target) {
            left = mid + 1; // ищем в правой половине
        } else {
            right = mid - 1; // ищем в левой половине
        }
    }
    
    return -1; // не найден
}
```

**Анализ**: На каждой итерации размер поиска уменьшается вдвое. Для массива из 1000 элементов требуется максимум 10 итераций (log₂(1000) ≈ 10).

**2. Поиск в сбалансированных деревьях**

```java
// Поиск в TreeSet (красно-чёрное дерево)
TreeSet<Integer> set = new TreeSet<>();
boolean contains = set.contains(42); // O(log n)

// Поиск в TreeMap
TreeMap<String, Integer> map = new TreeMap<>();
Integer value = map.get("key"); // O(log n)
```

**Почему O(log n)**: В сбалансированном двоичном дереве высотой h можно хранить 2^h - 1 элементов. Следовательно, h = log₂(n), где n — количество элементов.

**3. Работа с приоритетной очередью (двоичная куча)**

```java
PriorityQueue<Integer> pq = new PriorityQueue<>();
pq.offer(element); // O(log n) — всплытие элемента
pq.poll();         // O(log n) — погружение элемента
pq.peek();         // O(1) — доступ к корню
```

**Анализ**: При вставке элемент сравнивается с родителями по пути к корню (высота дерева = log n). При удалении элемент сравнивается с детьми по пути вниз.

**4. Возведение в степень за логарифмическое время**

```java
// Быстрое возведение в степень
public static long power(long base, int exp) {
    if (exp == 0) return 1;
    
    long half = power(base, exp / 2);
    
    if (exp % 2 == 0) {
        return half * half; // чётная степень
    } else {
        return base * half * half; // нечётная степень
    }
}
```

**Анализ**: Степень уменьшается вдвое на каждом рекурсивном вызове → O(log n).

**5. Навигация в skip list**

```java
// ConcurrentSkipListMap использует skip list
ConcurrentSkipListMap<Integer, String> skipList = new ConcurrentSkipListMap<>();
skipList.put(key, value);    // O(log n) в среднем
skipList.get(key);           // O(log n) в среднем
```

**Почему O(log n)**: Skip list — вероятностная структура данных с несколькими уровнями связных списков. В среднем поиск требует проверки O(log n) элементов.

#### Распознавание O(log n) в коде

**Признаки логарифмической сложности**:
1. Размер задачи делится на константу (обычно 2) на каждой итерации
2. Высота сбалансированного дерева
3. Количество разрядов числа в определённой системе счисления
4. Алгоритмы "разделяй и властвуй" без объединения результатов

**Примеры паттернов**:
```java
// Паттерн 1: деление области поиска
while (left < right) {
    int mid = (left + right) / 2;
    // ... работа с половиной данных
}

// Паттерн 2: деление числа
while (n > 1) {
    n = n / 2;
    // ... обработка
}

// Паттерн 3: уровни дерева
int height = (int) Math.ceil(Math.log(n) / Math.log(2));
```

### O(n) — линейная сложность

Время выполнения растёт линейно с размером входных данных.

**Примеры**:
```java
// Обход массива
for (int i = 0; i < array.length; i++) {
    process(array[i]); // O(n)
}

// Поиск элемента в несортированном массиве
public static int linearSearch(int[] array, int target) {
    for (int i = 0; i < array.length; i++) {
        if (array[i] == target) return i;
    }
    return -1;
}

// Вычисление суммы элементов
int sum = Arrays.stream(array).sum(); // O(n)
```

### O(n log n) — линеаризованная логарифмическая сложность

Комбинация линейной и логарифмической сложности. Характерна для эффективных алгоритмов сортировки.

**Примеры**:
```java
// Сортировка слиянием (Merge Sort)
Arrays.sort(array); // O(n log n) для примитивов (Dual-Pivot Quicksort)

// Сортировка объектов (TimSort — гибрид Merge Sort и Insertion Sort)
Collections.sort(list); // O(n log n)

// Heap Sort
PriorityQueue<Integer> pq = new PriorityQueue<>(array);
// Построение кучи: O(n)
// Извлечение всех элементов: n × O(log n) = O(n log n)
```

**Почему O(n log n)**:
- В алгоритме сортировки слиянием дерево рекурсии имеет глубину log n
- На каждом уровне выполняется O(n) операций слияния
- Итого: O(n) × O(log n) = O(n log n)

```java
// Упрощённая реализация Merge Sort
public static void mergeSort(int[] array, int left, int right) {
    if (left < right) {
        int mid = (left + right) / 2;
        mergeSort(array, left, mid);      // T(n/2)
        mergeSort(array, mid + 1, right); // T(n/2)
        merge(array, left, mid, right);   // O(n)
    }
    // T(n) = 2T(n/2) + O(n) = O(n log n)
}
```

### O(n²) — квадратичная сложность

Время выполнения растёт квадратично. Обычно связано с вложенными циклами.

**Примеры**:
```java
// Пузырьковая сортировка
public static void bubbleSort(int[] array) {
    for (int i = 0; i < array.length; i++) {
        for (int j = 0; j < array.length - 1 - i; j++) {
            if (array[j] > array[j + 1]) {
                swap(array, j, j + 1);
            }
        }
    }
}

// Поиск дубликатов наивным способом
public static boolean hasDuplicates(int[] array) {
    for (int i = 0; i < array.length; i++) {
        for (int j = i + 1; j < array.length; j++) {
            if (array[i] == array[j]) return true;
        }
    }
    return false;
}
```

**Современная практика**: Избегайте O(n²) для больших данных. Используйте более эффективные алгоритмы или структуры данных (например, HashSet для поиска дубликатов даёт O(n)).

### O(2ⁿ) — экспоненциальная сложность

Каждое увеличение n удваивает время выполнения. Практически неприменима для больших n.

**Примеры**:
```java
// Наивное решение задачи о рюкзаке
// Рекурсивное вычисление чисел Фибоначчи
public static int fibonacci(int n) {
    if (n <= 1) return n;
    return fibonacci(n - 1) + fibonacci(n - 2); // O(2ⁿ)
}

// Перебор всех подмножеств
public static void printSubsets(int[] array, int index, List<Integer> current) {
    if (index == array.length) {
        System.out.println(current);
        return;
    }
    // Не включаем элемент
    printSubsets(array, index + 1, current);
    // Включаем элемент
    current.add(array[index]);
    printSubsets(array, index + 1, current);
    current.remove(current.size() - 1);
}
```

**Оптимизация**: Используйте динамическое программирование (мемоизацию) для снижения сложности.

```java
// Фибоначчи с мемоизацией — O(n)
public static int fibonacciMemo(int n, Map<Integer, Integer> memo) {
    if (n <= 1) return n;
    if (memo.containsKey(n)) return memo.get(n);
    
    int result = fibonacciMemo(n - 1, memo) + fibonacciMemo(n - 2, memo);
    memo.put(n, result);
    return result;
}
```

## Пространственная сложность

**Пространственная сложность** описывает объём дополнительной памяти, необходимой для работы алгоритма.

**Примеры**:
```java
// O(1) — константная память
public static int sum(int[] array) {
    int total = 0; // одна переменная
    for (int num : array) {
        total += num;
    }
    return total;
}

// O(n) — линейная память
public static int[] copyArray(int[] array) {
    return Arrays.copyOf(array, array.length); // создаём новый массив
}

// O(log n) — память для стека рекурсии в бинарном поиске
public static int binarySearchRecursive(int[] array, int target, int left, int right) {
    if (left > right) return -1;
    int mid = (left + right) / 2;
    
    if (array[mid] == target) return mid;
    if (array[mid] < target) return binarySearchRecursive(array, target, mid + 1, right);
    return binarySearchRecursive(array, target, left, mid - 1);
}

// O(n) — стек рекурсии для обхода дерева
public static void inorderTraversal(TreeNode node) {
    if (node == null) return;
    inorderTraversal(node.left);  // стек растёт до высоты дерева
    System.out.println(node.val);
    inorderTraversal(node.right);
}
```

## Амортизированная сложность

**Амортизированный анализ** рассматривает среднюю стоимость операции в худшем случае для последовательности операций.

**Пример: ArrayList.add()**
```java
ArrayList<Integer> list = new ArrayList<>();
for (int i = 0; i < n; i++) {
    list.add(i); // O(1) амортизированная
}
```

**Анализ**:
- Большинство вставок выполняются за O(1)
- Когда массив заполнен, создаётся новый массив в 1.5 раза больше — O(n)
- Рост происходит редко (при размерах 10, 15, 22, 33, 49...)
- Амортизированная стоимость каждой вставки: O(1)

## Практические примеры анализа

### Пример 1: Поиск наибольшего общего делителя (НОД)

```java
// Алгоритм Евклида — O(log min(a, b))
public static int gcd(int a, int b) {
    while (b != 0) {
        int temp = b;
        b = a % b;
        a = temp;
    }
    return a;
}
```

**Анализ**: На каждой итерации остаток уменьшается как минимум вдвое → O(log n).

### Пример 2: Подсчёт количества установленных бит

```java
// O(k), где k — количество установленных бит
public static int countBits(int n) {
    int count = 0;
    while (n != 0) {
        n &= (n - 1); // сбрасывает младший установленный бит
        count++;
    }
    return count;
}
```

### Пример 3: Нахождение пересечения двух массивов

```java
// Вариант 1: вложенные циклы — O(n × m)
public static List<Integer> intersect1(int[] nums1, int[] nums2) {
    List<Integer> result = new ArrayList<>();
    for (int i = 0; i < nums1.length; i++) {
        for (int j = 0; j < nums2.length; j++) {
            if (nums1[i] == nums2[j]) {
                result.add(nums1[i]);
                break;
            }
        }
    }
    return result;
}

// Вариант 2: использование HashSet — O(n + m)
public static List<Integer> intersect2(int[] nums1, int[] nums2) {
    Set<Integer> set = new HashSet<>();
    for (int num : nums1) {
        set.add(num); // O(n)
    }
    
    List<Integer> result = new ArrayList<>();
    for (int num : nums2) {
        if (set.contains(num)) { // O(1)
            result.add(num);
        }
    }
    return result; // Итого: O(n + m)
}

// Вариант 3: сортировка и два указателя — O(n log n + m log m)
public static List<Integer> intersect3(int[] nums1, int[] nums2) {
    Arrays.sort(nums1); // O(n log n)
    Arrays.sort(nums2); // O(m log m)
    
    List<Integer> result = new ArrayList<>();
    int i = 0, j = 0;
    
    while (i < nums1.length && j < nums2.length) { // O(n + m)
        if (nums1[i] == nums2[j]) {
            result.add(nums1[i]);
            i++;
            j++;
        } else if (nums1[i] < nums2[j]) {
            i++;
        } else {
            j++;
        }
    }
    
    return result;
}
```

## Best practices

1. **Анализируйте сложность перед оптимизацией**
   - Измеряйте производительность реальных данных
   - Профилируйте код (JProfiler, YourKit, JFR)
   - Не оптимизируйте преждевременно

2. **Выбирайте правильные структуры данных**
   - Частый поиск → HashSet/HashMap (O(1))
   - Сортированные данные → TreeSet/TreeMap (O(log n))
   - Частые вставки/удаления с концов → ArrayDeque (O(1))
   - Приоритетная обработка → PriorityQueue (O(log n))

3. **Используйте библиотечные алгоритмы**
   - `Arrays.sort()`, `Collections.sort()` — оптимизированы для Java
   - `Arrays.binarySearch()` — для поиска в отсортированных данных
   - Stream API для декларативных операций

4. **Помните о константах**
   - O(1000n) может быть медленнее O(n log n) для небольших n
   - Учитывайте накладные расходы (создание объектов, автобоксинг)

5. **Документируйте сложность**
   ```java
   /**
    * Находит элемент в отсортированном массиве.
    * 
    * @param array отсортированный массив
    * @param target искомый элемент
    * @return индекс элемента или -1
    * @complexity O(log n) время, O(1) память
    */
   public static int binarySearch(int[] array, int target) {
       // ...
   }
   ```

6. **Знайте сложность стандартных операций**
   - `HashMap.get()` — O(1) в среднем, O(n) в худшем
   - `TreeMap.get()` — O(log n) гарантированно
   - `ArrayList.add(index, element)` — O(n) из-за сдвига
   - `LinkedList.get(index)` — O(n) из-за последовательного обхода

## Вопросы на собеседовании

1. **Что такое O-нотация и почему мы используем именно её?**
   
   O-нотация описывает верхнюю границу роста функции, отбрасывая константы и младшие термы. Используется для описания наихудшего случая — самого важного для гарантий производительности в production.

2. **Почему бинарный поиск работает за O(log n)?**
   
   На каждом шаге размер области поиска уменьшается вдвое. Для n элементов требуется максимум log₂(n) операций. Пример: в массиве из 1024 элементов максимум 10 итераций (2¹⁰ = 1024).

3. **Какая сложность у операции добавления в ArrayList?**
   
   O(1) амортизированная для добавления в конец. При переполнении массив копируется в новый размером примерно в 1.5 раза больше, что даёт O(n) в худшем случае, но это происходит редко.

4. **Как найти пересечение двух больших массивов максимально эффективно?**
   
   Зависит от контекста:
   - Если массивы неотсортированы и можно использовать дополнительную память: HashSet — O(n + m)
   - Если массивы уже отсортированы: два указателя — O(n + m)
   - Если нужна экономия памяти: сортировка + два указателя — O(n log n + m log m)

5. **В чём разница между O(n) и O(2n)?**
   
   С точки зрения асимптотической сложности разницы нет — обе записываются как O(n). Константные множители отбрасываются, так как при больших n они несущественны.

6. **Почему HashMap.get() работает за O(1), а не за O(n)?**
   
   Хеш-функция распределяет ключи по бакетам. Идеально — один элемент на бакет, поэтому O(1). В худшем случае (все элементы в одном бакете) — O(n), но это редкость при хорошей хеш-функции. Java 8+ превращает длинные цепочки в деревья, обеспечивая O(log n).

7. **Как оценить сложность рекурсивного алгоритма?**
   
   Составьте рекуррентное соотношение:
   - T(n) = T(n/2) + O(1) → O(log n) — бинарный поиск
   - T(n) = 2T(n/2) + O(n) → O(n log n) — merge sort
   - T(n) = T(n-1) + T(n-2) + O(1) → O(2ⁿ) — наивный Фибоначчи
   
   Используйте Master Theorem или дерево рекурсии для анализа.

8. **Какие операции в Java-коллекциях имеют O(log n)?**
   
   - TreeSet: add, remove, contains
   - TreeMap: put, get, remove
   - PriorityQueue: offer, poll
   - Навигационные методы NavigableSet/NavigableMap: higher, lower, floor, ceiling

9. **Что такое амортизированная сложность? Приведите пример.**
   
   Средняя стоимость операции в последовательности операций. Пример: ArrayList.add() — большинство вставок O(1), редкие расширения массива O(n), в среднем O(1).

10. **Как оптимизировать алгоритм с O(n²) до O(n log n)?**
    
    Зависит от задачи:
    - Поиск дубликатов: HashSet вместо вложенных циклов
    - Сортировка: QuickSort/MergeSort вместо BubbleSort
    - Поиск ближайших элементов: сортировка + скользящее окно
    - Подсчёт инверсий: модифицированный merge sort

---

> **Современная практика**: В современной разработке анализ сложности — обязательный навык. Системы обрабатывают миллионы запросов, и разница между O(n) и O(log n) критична. Используйте профилирование для валидации теоретических оценок и помните: "Преждевременная оптимизация — корень всех зол" (Дональд Кнут), но незнание сложности — путь к проблемам производительности.
